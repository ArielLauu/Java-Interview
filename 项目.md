## 项目总结

[TOC]

### 秒杀

#### 总结博客

项目回顾：https://blog.csdn.net/weixin_41891177/article/details/107775394

面经：https://blog.csdn.net/weixin_44406146/article/details/107800771

#### 如何解决库存超卖问题

秒杀场景下，并发会特别的大，有两种情况会导致库存卖超：

1. 一个用户同时发出了多个请求，如果库存足够，没加限制，用户就可以下多个订单。

2. 减库存的SQL上没有加库存数量的判断，并发的时候也会导致把库存减成负数。

对于`1.`前端加验证码，防止合法用户快速点鼠标同时发出多个请求，在后端的miaosha_order表中，对user_id和goods_id加唯一索引，确保就算是刷接口一个用户对一个商品绝对不会生成两个订单。

对于`2.`需要在扣减库存的SQL上加上库存数量的判断，只有扣减库存成功才可以生成订单：

![图片描述](https://typora-image-ariellauu.oss-cn-beijing.aliyuncs.com/uPic/5abc37b9000173f906870131.png)

---

#### 怎么使用ThreadLocal

- 在页面中很多地方都需要用于处于登录状态才能获取并操作该页面，我们无法为每一个controller中每个方法都执行获取用户的操作，我们可以在第一次登录时把用户保存到Cookie中或者session中

- **ThreadLocal存储user：**后端很多方法中都需要获取用户，我们没办法每次都去获取，我们可以在第一次获取到时把它保存到ThreadLocal中(一个用户操作对应一个线程，所以我们可以放心保存)，需要的时候直接拿出来使用

![image-20210216102527686](https://typora-image-ariellauu.oss-cn-beijing.aliyuncs.com/uPic/image-20210216102527686.png)

---

#### 秒杀库存如何设计的

- 商品表：很稳定，只包含商品的基本信息

- 秒杀商品表：商品基础信息+开始时间+结束时间+数量等

- 订单表：所有订单

- 秒杀订单表：秒杀相关的订单

如果每做一个活动，就在商品表添加字段标识一下，商品表会越来越难以维护，而且非常不稳定，惊颤会修改。订单表类似。所以创建新的秒杀商品表，通过商品id进行关联。

---

#### redis怎么用的，都存了什么

- **秒杀次数（防刷）**：对一个商品秒杀时，redis会记录某个用户对秒杀按钮的点击次数，超过5次直接返回”访问太频繁“提示
- **html（页面/url缓存）**
- **预加载的库存**：在秒杀的时候用来预减库存，防止大量无效请求穿透到数据库
- **秒杀path**：通过动态生成随机串UUID,结合用户ID和商品ID，生成path存入redis，并将path传给前端。前端获取path后，再根据path地址调用秒杀服务
- **验证码**：秒杀前需要输入验证码，作用为防刷限流，秒杀错峰
- **根据id和token缓存用户**：用于分布式session
- **秒杀订单**：判断用户是否已经下单，防止重复下单

---

#### 秒杀业务难点（待整理）

- 数据一致性：防止超卖和重复下单

---

#### 消息队列如何使用的

MQ 的核心场景：异步、解耦、削峰填谷

- **mq怎么用的？**

  Direct模式，Fanout模式，Topic模式，Header模

- **秒杀中mq的部分用到了什么设计模式**

  发布订阅模式

- **mq用了几个worker？如何保证顺序消费？**

  1. 为什么

  消息队列中的若干消息如果是对同一个数据进行操作，这些操作具有前后的关系，必须要按前后的顺序执行，否则就会造成数据异常

  2. 原因

  一个queue，有多个consumer去消费，这样就会造成顺序的错误，consumer从MQ里面读取数据是有序的，但是每个consumer的执行时间是不固定的，无法保证先读到消息的consumer一定先完成操作，这样就会出现消息并没有按照顺序执行，造成数据顺序错误

  一个queue，有多个consumer去消费，这样就会造成顺序的错误，consumer从MQ里面读取数据是有序的，但是每个consumer的执行时间是不固定的，无法保证先读到消息的consumer一定先完成操作，这样就会出现消息并没有按照顺序执行，造成数据顺序错误

  3. 保证顺序消费

  拆分多个queue，每个queue一个consumer，就是多一些queue而已，确实是麻烦点；这样也会造成吞吐量下降，可以在消费者内部采用多线程的方式取消费。

  或者就一个queue但是对应一个consumer，然后这个consumer内部用内存队列做排队，然后分发给底层不同的worker来处理

- **mq堵塞了怎么办？**



分布式锁怎么实现 什么时候加 加在什么上面

秒杀demo，秒杀业务都要哪些难点？对于这些难点都应该怎么解决（一直追问，用这个方法可能会有什么问题？数据量大到一定程度你的方法还行吗？等等等等）建议在准备项目的时候要对与项目的各个点要了如指掌，包括可能的缺陷等等

怎么保证用户不会重复秒秒杀？ 答：用redis的set结构记录已经参与过秒杀的用户。 追问：那如果有人用爬虫构造很多用户来刷你的秒杀，怎么办？ 答：在网关层做一个流量过滤器，分析流量特征，把流量特征异常的ip加入黑名单 

Redis怎么来保证它得可用性啊？ 集群。集群怎么实现啊？ （问了两次 ，可见这个部门redis用得很多）

3.关于秒杀项目的一系列问题:

(1)redis怎么用的，都存了什么？mq怎么用的？

(2)mq用了几个worker？如何保证顺序消费？

(3)压测的结论是什么？原因？

(4)如果持续增大并发量，哪个模块会出问题？如何解决？

(5)数据库主从复制的延时问题如何解决？

(6)mq堵塞了怎么办？

(7)多worker会不会对数据库产生压力？如何解决？

(8)如何避免重复下单？

(9)客户端如何知道自己下单成功？

4.设计模式知道哪些？

5.秒杀项目中用了什么设计模式？

6.秒杀中mq的部分用到了什么设计模式？

1.秒杀项目一系列问题（涉及到超卖，重复下单，数据不一致，交易系统，事务管理，客户端轮询反馈，第三方支付，数据库表的结构，各个表之间的关联等等）

秒杀系统介绍一下？为什么要引入redis？

秒杀系统压测了吗？如果继续增大并发量，那么接下来的瓶颈是什么？如何解决？（mq）

---

### RPC

**重要的部分**

1. 注册中心
2. 网络传输
3. 序列化和反序列化
4. 动态代理
5. 负载均衡
6. 传输协议

### 1. 项目相关

#### 介绍自己的RPC项目



---

#### 实现高性能的RPC关键在于哪些方面

选择合适的**传输协议**和**序列化协议**

---

#### 整体服务调用链路是怎样的

调用是通过动态代理实现的。首先基于动态代理生成代理服务对象，代理服务对象中包括`rpcClient`和服务属性。当调用代理对象的方法时，由代理进行相关信息（方法、参数等）的组装并发送到服务器进行远程调用，并由代理接收调用结果并返回

---

#### JDK动态代理机制是怎么实现的

1、编写需要被代理的类和接口

2、编写代理类，需要实现 `InvocationHandler` 接口，重写 `invoke()` 方法；

3、使用`Proxy.newProxyInstance(ClassLoader loader, Class<?>[] interfaces, InvocationHandler h)`动态创建代理类对象，通过代理类对象调用业务方法。

---

#### 如何实现跨语言的序列化或者rpc框架

一个rpc框架要想跨语言，本质是在解决**序列化/反序列化**的跨语言问题

**只要 RPC 框架保证在不同的编程语言中，使用相同的序列化协议，就可以实现跨语言的通信。**另外，为了在不同的语言中能描述相同的服务定义，跨语言的 RPC 框架还需要提供一套描述服务的语言，称为 IDL（Interface description language）。所有的服务都需要用 IDL 定义，再由 RPC 框架转换为特定编程语言的接口或者抽象类。这样，就可以实现跨语言调用了。



### 2. Netty

#### 为什么选用Netty来做通信框架

Netty 是一个异步事件驱动的网络应用程序框架，基于 NIO 的，封装了 JDK 的 NIO，让我们使用起来更加方法灵活。

使用Netty主要因为其以下特点和优势：

- **使用简单**：封装了 NIO 的很多细节，使用更简单。 
- **功能强大**：预置了多种编解码功能，支持多种主流协议。 
- **定制能力强**：可以通过 ChannelHandler 对通信框架进行灵活地扩展。 
- **性能高**：通过与其他业界主流的 NIO 框架对比，Netty的综合性能最优。 

**为什么不用NIO？**

- 不用NIO主要是因为NIO的编程模型复杂而且存在一些BUG，并且对编程功底要求比较高。而且，NIO在面对断连重连、包丢失、粘包等问题时处理过程非常复杂。

---

#### netty高性能主要依赖了哪些特性

- IO 线程模型：同步非阻塞，用最少的资源做更多的事。 

- 内存零拷贝：尽量减少不必要的内存拷贝，实现了更高效率的传输。 

- 内存池设计：申请的内存可以重用，主要指直接内存。内部实现是用一颗二叉查找树管理内存分配情况。 

- 串行化处理读写：避免使用锁带来的性能开销。 

- 高性能序列化协议：支持 protobuf 等高性能序列化协议。

---

#### Netty Bytebuf工作原理，和NIO ByteBuffer区别

**1. ByteBuffer**

- ByteBuffer长度固定，一旦分配完成，它的容量不能动态扩展和收缩，当需要编码的对象大于ByteBuffer的容量时，会发生索引越界异常；
- ByteBuffer只有一个标识位置的指针position，读写的时候需要手工调用flip()和rewind()等
- ByteBuffer的API功能有限

**2. ByteBuf**

- 长度可以实现动态扩展
- 通过两个位置指针来协助缓冲区的读写操作，读操作使用readerIndex，写操作使用writerIndex
- **内存分配：**可以分**为堆内存（HeapByteBuf）字节缓冲区**和**直接内存（DirectByteBuf）字节缓冲区**。直接内存缓冲区中分配和回收速度相对较慢，但是读写时少了一次内存复制，速度更快。
- **内存回收角度：**分为**基于对象池的ByteBuf**和**普通ByteBuf**，基于对象池的ByteBuf可以重用ByteBuf对象，它自己维护了一个内存池，可以循环利用创建的ByteBuf，提升内存的使用效率。

---

#### 还知道其他网络通信框架

Mina，和Netty同一个人设计的

与Mina相比有什么优势：

1. 都是Trustin Lee的作品，Netty更晚；
2. Mina将内核和一些特性的联系过于紧密，使得用户在不需要这些特性的时候无法脱离，相比下性能会有所下降，Netty解决了这个设计问题；
3. Netty的文档更清晰，很多Mina的特性在Netty里都有；
4. Netty更新周期更短，新版本的发布比较快；
5. 它们的架构差别不大，Mina靠apache生存，而Netty靠jboss，和jboss的结合度非常高，Netty有对google protocal buf的支持，有更完整的ioc容器支持(spring,guice,jbossmc和osgi)；
6. Netty比Mina使用起来更简单，Netty里你可以自定义的处理upstream events或/和downstream events，可以使用decoder和encoder来解码和编码发送内容；
7. Netty和Mina在处理UDP时有一些不同，Netty将UDP无连接的特性暴露出来；而Mina对UDP进行了高级层次的抽象，可以把UDP当成&quot;面向连接&quot;的协议，而要Netty做到这一点比较困难。
8. 从任务调度粒度，mina会将有IO任务的session写入队列中，当循环执行任务时，则会轮询所有的session，并依次把session中的所有任务取出来运行。这样粗粒度的调度是不公平调度，会导致某些请求的延迟很高。

---

#### 项目如果要实现内存零拷贝怎么做

Netty的内存零拷贝体现在：

1. **DIRECT BUFFERS（堆外内存）**：Netty 的接收和发送 ByteBuffer 采用 DIRECT BUFFERS，使用堆外直接内存进行 Socket 读写，不需要进行字节缓冲区的二次拷贝。如果使用传统的堆内存（HEAP BUFFERS）进行 Socket 读写，JVM 会将堆内存 Buffer 拷贝一份到直接内存中，然后才写入 Socket 中。相比于堆外直接内存，消息在发送过程中多了一次缓冲区的内存拷贝。
2. **CompositeByteBuf**：Netty 提供了组合 Buffer 对象，可以聚合多个 ByteBuffer 对象，用户可以像操作一个 Buffer 那样方便的对组合 Buffer 进行操作，这样就免去了重新分配空间再复制数据的开销。
3. **FileRegion类的transferTo()**：Netty 的文件传输采用了 transferTo 方法，它可以直接将文件缓冲区的数据发送到目标 Channel，避免了传统通过循环 write 方式导致的内存拷贝问题。

参考链接：https://zhuanlan.zhihu.com/p/88599349

---

#### 怎么实现序列化，Kryo原理

实现了两种序列化方式，一种是kryo，一种是Protobuf

**1. Kryo**

Kryo是专门专对Java语言的，性能非常好

- Kryo在做序列化时，也**没有记录属性的名称，而是给每个属性分配了一个id**，但是他却并没有GPB那样通过一个schema文件去做id和属性的一个映射描述，所以一旦我们修改了对象的属性信息，比如说新增了一个字段，那么Kryo进行反序列化时就可能发生属性值错乱甚至是反序列化失败的情况
- 由于Kryo没有序列化属性名称的描述信息，所以序列化/反序列化之前，需要先将要处理的类**在Kryo中进行注册**，这一操作在首次序列化时也会消耗一定的性能

**2. Protobuf**

出自Google，性能比较优秀，支持多种语言，同时跨平台。使用比较繁琐，需要自定义proto文件，用来完成Java对象中的基本数据类型和GPB自己定义的类型之间的一个映射。Java中通过与ProtoStuff的结合，使用更加简便了，无需手动编写proto文件

---

#### 除了Select，还有什么（Poll Epoll，区别是什么）





### 3. RPC（Dubbo）

#### 项目中负载均衡怎么实现的

实现了两种，**一致性哈希**和**随机**负载均衡算法

---

#### 负载均衡了解哪些

**1. RandomLoadBalance**：随机负载均衡算法，Dubbo默认的负载均衡策略

- 如果权重一致随机取出，如果不同则跟随机负载均衡一致，累加权重，然后随机取出

**2. RoundRobinLoadBalance**：轮询负载均衡算法，按公约后的权重设置轮循比率

- 参考自 Nginx 的平滑加权轮询负载均衡
- 过程：有两个参数，currentWeight和weight，currentWeight动态变化，weight固定。当有新的请求进来时，遍历服务器列表，让它的 currentWeight 加上自身权重。遍历完成后，找到最大的 currentWeight，并将其减去权重总和，然后返回相应的服务器即可
- **缺点**：在权重设置不合理的情况下，会**导致某些节点无法负载请求**，另外，如果有些机器性能比较低，**会存在请求阻塞的情况**

**3. LeastActiveLoadBalance**：最少活跃数负载均衡算法，会将请求负载到请求活跃数最少的节点上，如果节点上活跃数相同，则随机负载。

> 每个服务提供者对应一个活跃数 active。初始情况下，所有服务提供者活跃数均为0。每收到一个请求，活跃数加1，完成请求后则将活跃数减1。在服务运行一段时间后，性能好的服务提供者处理请求的速度更快，因此活跃数下降的也越快，此时这样的服务提供者能够优先获取到新的服务请求、这就是最小活跃数负载均衡算法的基本思想

**4. ConsistentHashLoadBalance**: 一致性Hash，相同参数的请求总是发到同一提供者。当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动

- 构造函数中，每个实际的提供者均有160个（默认值，可调整）虚拟节点，每个提供者对应的虚拟节点将平均散列到哈希环上，当有请求时，先计算该请求参数对应的哈希值，然后顺时针寻找最近的虚拟节点，得到实际的提供者节点。

https://dubbo.apache.org/zh/docs/v2.7/dev/source/loadbalance/

---

#### 一致性哈希比其他方法的优点？

- 让虚拟节点在圆环上分散开来，避免数据倾斜。（数据倾斜是指，由于节点不够分散，导致大量请求落到了同一个节点上，而其他节点只会接收到了少量请求的情况）
- 在一些需要将同一个请求参数对应到同个服务的场景下很适合

- 当对实例进行增加或者删除的时候，不会造成哈希表的全量重新映射，而是增量式的重新映射，能避免引起很大的变动

---

#### 一致性哈希在某节点宕机时怎么保证一致性的

某服务节点宕机，服务会被影射到在环中，其前一个虚拟节点对应的服务提供者上

---

#### 除了Dubbo，还了解其他RPC框架吗（gRPC, Thrift, Dubbo 有什么区别）

**1. Dubbo**

由阿里开源，后面加入Apache，提供了三大核心能力：

- 面向接口的远程方法调用
- 智能容错和负载均衡
- 服务自动注册和发现

**2. gRPC**

gRPC是谷歌开源的，高性能的、通用的RPC框架。其主要面向移动应用开发，并基于HTTP2协议标准而设计，基于Protobuf序列化协议开发，支持众多语言

**3. Thrift**

是FaceBook开源的跨语言RPC框架，支持多种不同的编程语言

**对比：**

gRPC和Thrift虽然支持跨语言的调用，但是只提供了基本的RPC框架功能，缺乏一系列配套的服务话组件和服务治理功能的支撑。Dubbo不论是功能完善程度、生态系统还是社区活跃度，都很优秀。

但是Dubbo跨语言支持一般，主要还是支持Java

---

#### dubbo不足，以及优化方向

**服务提供方与调用方接口依赖方式太强**：我们为每个微服务定义了各自的service抽象接口，并通过持续集成发布到私有仓库中，调用方应用对微服务提供的抽象接口存在强依赖关系，因此不论开发、测试、集成环境都需要严格的管理版本依赖，才不会出现服务方与调用方的不一致导致应用无法编译成功等一系列问题，以及这也会直接影响本地开发的环境要求，往往一个依赖很多服务的上层应用，每天都要更新很多代码并install之后才能进行后续的开发。若没有严格的版本管理制度或开发一些自动化工具，这样的依赖关系会成为开发团队的一大噩梦。而REST接口相比RPC更为轻量化，服务提供方和调用方的依赖只是依靠一纸契约，不存在代码级别的强依赖，当然REST接口也有痛点，因为接口定义过轻，很容易导致定义文档与实际实现不一致导致服务集成时的问题，但是该问题很好解决，只需要通过每个服务整合swagger，让每个服务的代码与文档一体化，就能解决。所以在分布式环境下，REST方式的服务依赖要比RPC方式的依赖更为灵活。

**服务对平台敏感，难以简单复用**：通常我们在提供对外服务时，都会以REST的方式提供出去，这样可以实现跨平台的特点，任何一个语言的调用方都可以根据接口定义来实现。那么在Dubbo中我们要提供REST接口时，不得不实现一层代理，用来将RPC接口转换成REST接口进行对外发布。若我们每个服务本身就以REST接口方式存在，当要对外提供服务时，主要在API网关中配置映射关系和权限控制就可实现服务的复用了。



### 4. Zookeeper

#### 服务注册中心怎么做的（服务注册原理）

通过zookeeper实现的服务注册和发现

服务注册的时候，将完整的服务名称rpcServiceName(className+group+version)作为根节点，子节点是对应的服务地址(ip+端口号)。如果相同服务被部署多份，一个根节点回对应多个子节点。当我们要获取某个服务的对应地址，直接根据完整服务名获得其下所有子节点，然后根据负载均衡策略取出一个就行了。同时也会给该服务节点的子节点注册监听器，在节点的子节点发生变化时，可以自定义回调操作。

> group：用于处理一个接口有多个实现类
>
> version：为后续服务版本不兼容升级提供可能

---

#### 为什么用Zookeeper做注册中心(优点，与其他选型对比下)



---

#### Zookeeper角色

三种角色，分为Leader、Follower和Observer

- ZooKeeper 集群中的所有机器通过一个 **Leader 选举过程** 来选定一台称为 “**Leader**” 的机器，Leader 可以提供读写服务。

- 除了 Leader 外，**Follower** 和 **Observer** 都只能提供读服务。
- 此外，**Observer** 机器不参与 Leader 选举和“过半写成功”策略，因此 Observer 机器可以在不影响写性能的情况下提升集群的读性能。

![img](https://snailclimb.gitee.io/javaguide/docs/system-design/distributed-system/zookeeper/images/zookeeper集群中的角色.png)

| 角色     | 说明                                                         |
| -------- | ------------------------------------------------------------ |
| Leader   | 提供读和写，负责投票的发起和决议，更新系统状态。             |
| Follower | 提供读服务，如果是写服务则转发给 Leader。在选举过程中参与投票。 |
| Observer | 提供读服务器，如果是写服务则转发给 Leader。不参与选举投票和与“过半写成功”策略。在不影响写性能的情况下提升集群的读性能。 |

##### Leader选举过程

1. **Leader election（选举阶段）**：节点在一开始都处于选举阶段，只要有一个节点得到超半数节点的票数，它就可以当选准 leader。
2. **Discovery（发现阶段）** ：在这个阶段，followers 跟准 leader 进行通信，同步 followers 最近接收的事务提议。
3. **Synchronization（同步阶段）** :同步阶段主要是利用 leader 前一阶段获得的最新提议历史，同步集群中所有的副本。同步完成之后 准 leader 才会成为真正的 leader。
4. **Broadcast（广播阶段）** :到了这个阶段，ZooKeeper 集群才能正式对外提供事务服务，并且 leader 可以进行消息广播。同时如果有新的节点加入，还需要对新节点进行同步。

---

### 分布式数据一致性协议

#### 2PC两阶段提交协议

**一、准备阶段（prepare）**

1. 协调者节点向所有参与者节点询问是否可以执行提交操作（prepare请求），并开始等待各参与者节点的响应。
2. 参与者节点执行询问发起为止的所有事务操作，并将Undo信息和Redo信息写入日志
3. 各参与者节点响应协调者节点发起的询问。如果参与者节点的事务操作实际执行成功，则它返回一个"同意"消息；如果参与者节点的事务操作实际执行失败，则它返回一个"中止"消息

**二、提交阶段（commit）**

**成功**

当协调者节点从所有参与者节点获得的响应消息都为"同意"时：

1. 协调者节点向所有参与者节点发出"**正式提交**"的请求。
2. 参与者节点正式完成操作，并释放在整个事务期间内占用的资源。
3. 参与者节点向协调者节点发送"完成"消息。
4. 协调者节点收到所有参与者节点反馈的"完成"消息后，完成事务。

**失败**

如果任一参与者节点在第一阶段返回的响应消息为"终止"，或者协调者节点在第一阶段的询问超时之前无法获取所有参与者节点的响应消息时：

1. 协调者节点向所有参与者节点发出"**回滚操作**"的请求。
2. 参与者节点利用之前写入的Undo信息执行回滚，并释放在整个事务期间内占用的资源。
3. 参与者节点向协调者节点发送"回滚完成"消息。
4. 协调者节点收到所有参与者节点反馈的"回滚完成"消息后，取消事务。

**2PC缺点：**

1. 执行过程中，**所有参与节点都是事务阻塞型的**。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态
2. **协调者发生故障**：参与者会一直阻塞下去。因为参与者没有超时机制
3. **二阶段无法解决的问题**：协调者在发出 “正式提交” 消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交

https://zh.wikipedia.org/wiki/%E4%BA%8C%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4

---

#### 3PC三阶段提交协议

**3PC和2PC的区别：**

- 对于协调者[Coordinator]和参与者[Cohort]都设置了超时机制（在2PC中，只有协调者拥有超时机制，即如果在一定时间内没有收到cohort的消息则默认失败）。
- 在2PC的准备阶段和提交阶段之间，插入预提交阶段，使3PC拥有CanCommit、PreCommit、DoCommit三个阶段。说白了，PreCommit是一个缓冲，保证了在最后提交阶段之前各参与节点的状态是一致的。

**一、CanCommit阶段**

1. **事务询问**：Coordinator向Cohort发送CanCommit请求。询问是否可以执行事务提交操作。然后开始等待参与者的响应。
2. **响应反馈**：Cohort接到CanCommit请求之后，正常情况下，如果其自身可以顺利执行事务，则返回Yes响应，并进入预备状态。否则反馈No

**二、PreCommit阶段**

Coordinator根据Cohort的反应情况来决定是否可以继续事务的PreCommit操作。有以下两种可能：

**事务的预执行**：如果Coordinator从所有的Cohort获得的反馈都是Yes响应

1. 发送预提交请求。Coordinator向Cohort发送PreCommit请求，并进入Prepared阶段。
2. 事务预提交。Cohort接收到PreCommit请求后，会执行事务操作，并将undo和redo信息记录到事务日志中。
3. 响应反馈。如果Cohort成功的执行了事务操作，则返回ACK响应，同时开始等待最终指令。

**中断事务**：如果有任何一个Cohort向Coordinator发送了No响应，或者等待超时之后，Coordinator都没有接到Cohort的响应

1. 发送中断请求。Coordinator向所有Cohort发送abort请求。
2. 中断事务。Cohort收到来自Coordinator的abort请求之后（或超时之后，仍未收到Cohort的请求），执行事务的中断。

**三、DoCommit阶段**

该阶段进行真正的事务提交，也可以分为以下两种情况。

**执行提交**

1. 发送提交请求。Coordinator接收到Cohort发送的ACK响应，那么他将从预提交状态进入到提交状态。并向所有Cohort发送doCommit请求。
2. 事务提交。Cohort接收到doCommit请求之后，执行正式的事务提交。并在完成事务提交之后释放所有事务资源。
3. 响应反馈。事务提交完之后，向Coordinator发送ACK响应。
4. 完成事务。Coordinator接收到所有Cohort的ACK响应之后，完成事务。

**中断事务**

Coordinator没有接收到Cohort发送的ACK响应（可能是接受者发送的不是ACK响应，也可能响应超时），那么就会执行中断事务。

1. 发送中断请求。Coordinator向所有Cohort发送abort请求
2. 事务回滚。Cohort接收到abort请求之后，利用其在阶段二记录的undo信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源。
3. 反馈结果。Cohort完成事务回滚之后，向Coordinator发送ACK消息
4. 中断事务。Coordinator接收到参与者反馈的ACK消息之后，执行事务的中断。

**如何解决2PC的问题**

**在doCommit阶段，如果Cohort无法及时接收到来自Coordinator的doCommit或者abort请求时，会在等待超时之后，会继续进行事务的提交**。（其实这个应该是基于概率来决定的，当进入第三阶段时，说明参与者在第二阶段已经收到了PreCommit请求，那么Coordinator产生PreCommit请求的前提条件是他在第二阶段开始之前，收到所有参与者的CanCommit响应都是Yes。一旦参与者收到了PreCommit，意味他知道大家其实都同意修改了。所以，一句话概括就是，当进入第三阶段时，由于网络超时等原因，虽然参与者没有收到commit或者abort响应，但是他有理由相信：成功提交的几率很大。）

**缺点：**

如果进入PreCommit后，Coordinator发出的是abort请求，如果只有一个Cohort收到并进行了abort操作，而其他对于系统状态未知的Cohort会根据3PC选择继续Commit，那么系统的不一致性就存在了。所以无论是2PC还是3PC都存在问题，后面会继续了解那个传说中唯一的一致性算法Paxos

https://csruiliu.github.io/blog/20160530-intro-3pc/

---

#### Paxos

Paxos算法解决的问题是一个分布式系统如何就某个值达成一致。

Paxos中一共有三种角色，proposers，acceptors，和 learners。

- proposers 提出提案，提案信息包括提案编号和提议的 value；
- acceptor 收到提案后可以接受（accept）提案，若提案获得多数派（majority）的 acceptors 的接受，则称该提案被批准（chosen）；
- learners 只能“学习”被批准的提案

**算法内容：**

- 为了满足P2c的约束，proposer提出一个提案前，首先要和足以形成多数派的acceptors进行通信，获得他们进行的最近一次接受（accept）的提案（prepare过程），之后根据回收的信息决定这次提案的value，形成提案开始投票。

- 当获得多数acceptors接受（accept）后，提案获得批准（chosen），由acceptor将这个消息告知learner。这个简略的过程经过进一步细化后就形成了Paxos算法。

> **P2c**：如果一个编号为 n 的提案具有 value v，该提案被提出（issued），那么存在一个多数派，要么他们中所有人都没有接受（accept）编号小于 n 的任何提案，要么他们已经接受（accept）的所有编号小于 n 的提案中编号最大的那个提案具有 value v。

通过一个决议分为两个阶段：

**一、prepare阶段**

1. proposer选择一个提案编号n并将prepare请求发送给acceptors中的一个多数派；
2. acceptor收到prepare消息后：
   - 如果提案的编号大于它已经回复的所有prepare消息(回复消息表示接受accept)，则acceptor将自己上次接受的提案回复给proposer，并承诺不再回复小于n的提案。
   - 如果一个acceptor发现存在一个更高编号的提案，则需要通知proposer，提醒其中断这次提案。

**二、批准阶段**

1. 当一个proposer收到了多数acceptors对prepare的回复后，就进入批准阶段。它要向回复prepare请求的acceptors发送accept请求，包括编号n和根据P2c决定的value（如果根据P2c没有已经接受的value，那么它可以自由决定value）。
2. 在不违背自己向其他proposer的承诺的前提下，acceptor收到accept请求后即批准这个请求。

https://zh.wikipedia.org/zh-cn/Paxos%E7%AE%97%E6%B3%95#.E5.AE.9E.E4.BE.8B

---

#### ZAB

ZooKeeper 并没有完全采用 Paxos算法 ，而是使用 ZAB 协议作为其保证数据一致性的核心算法。另外，在ZooKeeper的官方文档中也指出，ZAB协议并不像 Paxos 算法那样，是一种通用的分布式一致性算法，它是一种特别为Zookeeper设计的崩溃可恢复的原子消息广播算法。

**ZAB 协议介绍**

ZAB（ZooKeeper Atomic Broadcast 原子广播） 协议是为分布式协调服务 ZooKeeper 专门设计的一种支持崩溃恢复的原子广播协议。 在 ZooKeeper 中，主要依赖 ZAB 协议来实现分布式数据一致性，基于该协议，ZooKeeper 实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。

**协议两种基本的模式：崩溃恢复和消息广播**

- **崩溃恢复** ：当整个服务框架在启动过程中，或是当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB 协议就会进入恢复模式并选举产生新的Leader服务器。当选举产生了新的 Leader 服务器，同时集群中已经有过半的机器与该Leader服务器完成了状态同步之后，ZAB协议就会退出恢复模式。其中，**所谓的状态同步是指数据同步，用来保证集群中存在过半的机器能够和Leader服务器的数据状态保持一致**。
- **消息广播** ：**当集群中已经有过半的Follower服务器完成了和Leader服务器的状态同步，那么整个服务框架就可以进入消息广播模式了。** 当一台同样遵守ZAB协议的服务器启动后加入到集群中时，如果此时集群中已经存在一个Leader服务器在负责进行消息广播，那么新加入的服务器就会自觉地进入数据恢复模式：找到Leader所在的服务器，并与其进行数据同步，然后一起参与到消息广播流程中去。



Raft了不了解

了不不了解hessian协议（序列化协议）

有没有提供什么扩展的接口，钩子给其人或程序方便扩展

微服务的技术发展方向

服务治理相关的事情

Zookeeper集群节点宕机了怎么发现剔除

https://www.nowcoder.com/discuss/588903?type=0&order=1&pos=6&page=1&channel=1009&source_id=discuss_center_0_nctrack